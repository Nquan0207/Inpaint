{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# Inpainting From Scratch Pipeline (Notebook)\n",
        "\n",
        "Input image (occluded) → Mask (occlusion ∩ object) → Train inpainting model (from scratch) → Output → Evaluate (PSNR, SSIM, LPIPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import os, math, random\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import ToTensor as ToTensorTorchvision\n",
        "\n",
        "from skimage.metrics import structural_similarity as skimage_ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as skimage_psnr\n",
        "\n",
        "try:\n",
        "    import lpips  # perceptual metric\n",
        "except Exception:\n",
        "    lpips = None\n",
        "\n",
        "# Reuse preprocessing (binarize mask, pad, protect borders)\n",
        "from inpainting import preprocess_img_and_mask\n",
        "from seg_maskcnn import get_device as seg_get_device, _model as seg_model\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.backends.mps.is_available():\n",
        "        return torch.device('mps')\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "device = get_device()\n",
        "print('Using device:', device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np_random_seed = np.random.seed\n",
        "np_random_seed(42)\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "utils",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utils: tensor conversion, mask intersection, optional pet-mask prediction\n",
        "def img_to_tensor(img: Image.Image) -> torch.Tensor:\n",
        "    arr = np.array(img).astype(np.float32) / 255.0\n",
        "    if arr.ndim == 2:\n",
        "        arr = arr[..., None]\n",
        "    arr = np.transpose(arr, (2, 0, 1))\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "def tensor_to_pil(t: torch.Tensor) -> Image.Image:\n",
        "    t = t.detach().clamp(0,1)\n",
        "    arr = (t.permute(1,2,0).cpu().numpy()*255.0).astype(np.uint8)\n",
        "    if arr.shape[2] == 1:\n",
        "        arr = arr[...,0]\n",
        "    return Image.fromarray(arr)\n",
        "\n",
        "def safe_intersect_masks(occ_mask: Image.Image, pet_mask: Optional[Image.Image]) -> Image.Image:\n",
        "    m_occ = (np.array(occ_mask.convert('L')) > 127).astype(np.uint8)\n",
        "    if pet_mask is None:\n",
        "        return Image.fromarray((m_occ*255).astype(np.uint8), 'L')\n",
        "    m_pet = (np.array(pet_mask.convert('L')) > 127).astype(np.uint8)\n",
        "    inter = (m_occ & m_pet).astype(np.uint8)\n",
        "    if inter.sum() == 0:\n",
        "        inter = m_occ\n",
        "    return Image.fromarray((inter*255).astype(np.uint8), 'L')\n",
        "\n",
        "def predict_pet_mask_safe(img_pil: Image.Image) -> Optional[Image.Image]:\n",
        "    try:\n",
        "        tr = ToTensorTorchvision()\n",
        "        x = tr(img_pil).unsqueeze(0).to(seg_get_device())\n",
        "        with torch.no_grad():\n",
        "            out = seg_model(x)[0]\n",
        "        labels = out['labels'].tolist()\n",
        "        scores = out['scores'].tolist()\n",
        "        masks = out['masks']\n",
        "        keep = [(l in [17,18]) and (s >= 0.5) for l,s in zip(labels, scores)]\n",
        "        if not any(keep):\n",
        "            return None\n",
        "        m = (masks[keep].squeeze(1) > 0.5).any(dim=0).float().cpu().numpy()\n",
        "        return Image.fromarray((m*255).astype(np.uint8), 'L')\n",
        "    except Exception as e:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset: occluded image + (occlusion ∩ pet) mask, target = clean image\n",
        "class OccludedPetDataset(Dataset):\n",
        "    def __init__(self, base_img_dir='data/oxford-iiit-pet/images', occluded_dir='data/occluded', resize=256):\n",
        "        self.base_img_dir = base_img_dir\n",
        "        self.occluded_dir = occluded_dir\n",
        "        self.resize = resize\n",
        "        # Only take occluded images, skip mask files in the same folder\n",
        "        self.files = [\n",
        "            f for f in os.listdir(occluded_dir)\n",
        "            if f.lower().endswith(('.jpg', '.png')) and '_mask' not in f.lower()\n",
        "        ]\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        self.resize_img = T.Resize((resize, resize), interpolation=T.InterpolationMode.BICUBIC)\n",
        "        self.resize_mask = T.Resize((resize, resize), interpolation=T.InterpolationMode.NEAREST)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "        occ_path = os.path.join(self.occluded_dir, name)\n",
        "        occ_mask_path = os.path.join(self.occluded_dir, name.rsplit('.',1)[0] + '_mask.png')\n",
        "        clean_path = os.path.join(self.base_img_dir, name.rsplit('.',1)[0] + '.jpg')\n",
        "\n",
        "        occ_img = Image.open(occ_path).convert('RGB')\n",
        "        clean_img = Image.open(clean_path).convert('RGB')\n",
        "        occ_mask = Image.open(occ_mask_path).convert('L')\n",
        "\n",
        "        # Disable pet mask for stability/speed; use occlusion mask only\n",
        "        pet_mask =  None             #predict_pet_mask_safe(occ_img)\n",
        "        inpaint_mask = safe_intersect_masks(occ_mask, pet_mask)\n",
        "\n",
        "        occ_img_p, inpaint_mask_p = preprocess_img_and_mask(occ_img, inpaint_mask)\n",
        "        # Guard: if mask covers ~all image, erode to avoid gray outputs\n",
        "        _arr = np.array(inpaint_mask_p)\n",
        "        _ratio = (_arr == 255).mean()\n",
        "        if _ratio > 0.95:\n",
        "            _arr = cv2.erode(_arr, np.ones((15,15), np.uint8), 1)\n",
        "            inpaint_mask_p = Image.fromarray(_arr, 'L')\n",
        "        clean_img_p, _ = preprocess_img_and_mask(clean_img, inpaint_mask)\n",
        "\n",
        "        occ_img_r = self.resize_img(occ_img_p)\n",
        "        clean_img_r = self.resize_img(clean_img_p)\n",
        "        mask_r = self.resize_mask(inpaint_mask_p)\n",
        "\n",
        "        occ_t = self.to_tensor(occ_img_r)\n",
        "        clean_t = self.to_tensor(clean_img_r)\n",
        "        mask_t = (self.to_tensor(mask_r) > 0.5).float()[:1]\n",
        "\n",
        "        # Zero-out the masked region in the network input (remove gray occluder)\n",
        "        occ_in_blur = occ_t.clone()\n",
        "        occ_in_blur[:, mask_t[0] > 0.5] = 0.0\n",
        "        x = torch.cat([occ_in_blur, mask_t], dim=0)  # [4,H,W]\n",
        "        y = clean_t\n",
        "        # Also return original occluded RGB (for visualization/blending)\n",
        "        return {'input': x, 'target': y, 'mask': mask_t, 'name': name, 'occ_rgb': occ_t}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b87e6734",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, use_bn=True):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=not use_bn),\n",
        "        ]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(out_ch))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        layers.append(\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=not use_bn)\n",
        "        )\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(out_ch))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Non-Local Self-Attention (SAGAN style)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
        "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
        "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "\n",
        "        q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)  # B,N,Cq\n",
        "        k = self.key(x).view(B, -1, H * W)                     # B,Ck,N\n",
        "        attn = torch.bmm(q, k)                                 # B,N,N\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        v = self.value(x).view(B, C, H * W)                    # B,C,N\n",
        "        out = torch.bmm(v, attn.permute(0, 2, 1))              # B,C,N\n",
        "        out = out.view(B, C, H, W)\n",
        "\n",
        "        return self.gamma * out + x\n",
        "\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_ch=4, out_ch=3, base=64):\n",
        "        \"\"\"\n",
        "        UNet nhẹ hơn + BatchNorm + Self-Attention ở bottleneck\n",
        "        - base=64 → 64-128-256-512\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.down1 = DoubleConv(in_ch, base)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.down2 = DoubleConv(base, base * 2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.down3 = DoubleConv(base * 2, base * 4)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck: DoubleConv + SelfAttention\n",
        "        self.mid = nn.Sequential(\n",
        "            DoubleConv(base * 4, base * 8),\n",
        "            SelfAttention(base * 8)\n",
        "        )\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(base * 8, base * 4, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(base * 8, base * 4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(base * 4, base * 2, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(base * 4, base * 2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(base * 2, base, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(base * 2, base)\n",
        "\n",
        "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        p1 = self.pool1(d1)\n",
        "\n",
        "        d2 = self.down2(p1)\n",
        "        p2 = self.pool2(d2)\n",
        "\n",
        "        d3 = self.down3(p2)\n",
        "        p3 = self.pool3(d3)\n",
        "\n",
        "        m = self.mid(p3)\n",
        "\n",
        "        u3 = self.up3(m)\n",
        "        c3 = torch.cat([u3, d3], dim=1)\n",
        "        d3 = self.dec3(c3)\n",
        "\n",
        "        u2 = self.up2(d3)\n",
        "        c2 = torch.cat([u2, d2], dim=1)\n",
        "        d2 = self.dec2(c2)\n",
        "\n",
        "        u1 = self.up1(d2)\n",
        "        c1 = torch.cat([u1, d1], dim=1)\n",
        "        d1 = self.dec1(c1)\n",
        "\n",
        "        out = self.outc(d1)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "47748c5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    PatchGAN + SpectralNorm\n",
        "    Input: 3ch RGB\n",
        "    Output: N×N patch realism map\n",
        "    \"\"\"\n",
        "    def __init__(self, ch=64):\n",
        "        super().__init__()\n",
        "\n",
        "        def block(in_c, out_c, use_bn=True):\n",
        "            layers = [\n",
        "                nn.utils.spectral_norm(nn.Conv2d(in_c, out_c, 4, 2, 1))\n",
        "            ]\n",
        "            if use_bn:\n",
        "                layers.append(nn.BatchNorm2d(out_c))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(3, ch, use_bn=False),\n",
        "            *block(ch, ch * 2),\n",
        "            *block(ch * 2, ch * 4),\n",
        "            *block(ch * 4, ch * 8),\n",
        "            nn.utils.spectral_norm(nn.Conv2d(ch * 8, 1, 4, 1, 1))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "train-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config, loaders, and losses\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    base_img_dir: str = '../data/oxford-iiit-pet/images'\n",
        "    occluded_dir: str = '../data/occluded'\n",
        "    resize: int = 256\n",
        "    batch_size: int = 4\n",
        "    epochs: int = 20\n",
        "    lr: float = 3e-4\n",
        "    val_split: float = 0.2\n",
        "    num_workers: int = 0\n",
        "    lpips_weight: float = 0.1  # set >0 to enable LPIPS term\n",
        "\n",
        "\n",
        "def masked_l1(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "    # mask: 1=inpaint region\n",
        "    w = mask\n",
        "    return (torch.abs(pred - target) * w).sum() / (w.sum() + 1e-6)\n",
        "\n",
        "\n",
        "def build_loaders(cfg: TrainConfig) -> Tuple[DataLoader, DataLoader]:\n",
        "    ds = OccludedPetDataset(cfg.base_img_dir, cfg.occluded_dir, resize=cfg.resize)\n",
        "    print(\"Total samples:\", len(ds))\n",
        "    n_val = max(1, int(len(ds) * cfg.val_split))\n",
        "    n_train = max(1, len(ds) - n_val)\n",
        "    train_ds, val_ds = random_split(ds, [n_train, n_val])\n",
        "    train_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "    val_dl = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "    print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))\n",
        "    return train_dl, val_dl\n",
        "\n",
        "\n",
        "def init_lpips(device):\n",
        "    if lpips is None:\n",
        "        return None\n",
        "    try:\n",
        "        net = lpips.LPIPS(net='alex').to(device).eval()\n",
        "        return net\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "train-eval",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Train and evaluate\n",
        "# -------------------------\n",
        "def train_one_epoch(\n",
        "    model, D,\n",
        "    opt_G, opt_D,\n",
        "    train_dl, device,\n",
        "    lpips_net=None, lpips_w=0.0\n",
        "):\n",
        "    model.train()\n",
        "    D.train()\n",
        "    total = 0.0\n",
        "\n",
        "    criterion_gan = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for batch in train_dl:\n",
        "        x = batch['input'].to(device)\n",
        "        y = batch['target'].to(device)\n",
        "        m = batch['mask'].to(device)\n",
        "\n",
        "        # =======================================================\n",
        "        # 1) GENERATOR FORWARD\n",
        "        # =======================================================\n",
        "        y_hat = model(x)\n",
        "\n",
        "        # reconstruction inside mask\n",
        "        l1_in = masked_l1(y_hat, y, m)\n",
        "        id_loss = torch.mean(torch.abs(y_hat - x[:, :3]) * (1 - m))\n",
        "\n",
        "        # lpips\n",
        "        lp_loss = 0\n",
        "        if lpips_net is not None and lpips_w > 0:\n",
        "            lp_loss = lpips_net(y_hat * 2 - 1, y * 2 - 1).mean()\n",
        "\n",
        "        # final blended output (for D and for realism)\n",
        "        final_fake = y_hat * m + x[:, :3] * (1 - m)\n",
        "\n",
        "        # =======================================================\n",
        "        # 2) TRAIN DISCRIMINATOR\n",
        "        # =======================================================\n",
        "        opt_D.zero_grad(set_to_none=True)\n",
        "\n",
        "        real_out = D(y)             # ground truth clean\n",
        "        fake_out = D(final_fake.detach())  # blended prediction\n",
        "\n",
        "        d_loss_real = criterion_gan(real_out, torch.ones_like(real_out))\n",
        "        d_loss_fake = criterion_gan(fake_out, torch.zeros_like(fake_out))\n",
        "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # =======================================================\n",
        "        # 3) TRAIN GENERATOR (GAN Loss)\n",
        "        # =======================================================\n",
        "        opt_G.zero_grad(set_to_none=True)\n",
        "\n",
        "        fake_out = D(final_fake)\n",
        "        gan_loss = criterion_gan(fake_out, torch.ones_like(fake_out))\n",
        "\n",
        "        loss = (\n",
        "            2.0 * l1_in +\n",
        "            0.005 * id_loss +\n",
        "            lpips_w * lp_loss +\n",
        "            0.01 * gan_loss\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
        "\n",
        "    return total / max(1, len(train_dl.dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3d2fd462",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, val_dl, device):\n",
        "    model.eval()\n",
        "    psnr_vals, ssim_vals = [], []\n",
        "    for batch in val_dl:\n",
        "        x = batch['input'].to(device)\n",
        "        y = batch['target'].to(device)\n",
        "        m = batch['mask'].to(device)\n",
        "        y_hat = model(x)\n",
        "        # Blend outside mask for fair evaluation\n",
        "        final = y_hat * m + x[:, :3] * (1 - m)\n",
        "        y_np = y.cpu().numpy()\n",
        "        yhat_np = final.cpu().numpy()\n",
        "        m_np = m.cpu().numpy()\n",
        "        B = y_np.shape[0]\n",
        "        for i in range(B):\n",
        "            gt = (y_np[i].transpose(1, 2, 0) * 255.0).astype(np.uint8)\n",
        "            pr = (yhat_np[i].transpose(1, 2, 0) * 255.0).astype(np.uint8)\n",
        "            mm = (m_np[i][0] > 0.5).astype(np.uint8)\n",
        "            if mm.sum() == 0:\n",
        "                continue\n",
        "            pr_masked = pr.copy()\n",
        "            gt_masked = gt.copy()\n",
        "            pr_masked[mm == 0] = gt[mm == 0]\n",
        "            psnr_vals.append(skimage_psnr(gt_masked, pr_masked, data_range=255))\n",
        "            ssim_vals.append(skimage_ssim(gt_masked, pr_masked, channel_axis=2, data_range=255))\n",
        "    mean_psnr = float(np.mean(psnr_vals)) if psnr_vals else 0.0\n",
        "    mean_ssim = float(np.mean(ssim_vals)) if ssim_vals else 0.0\n",
        "    return mean_psnr, mean_ssim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "run-train",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 999\n",
            "Train: 800 Val: 199\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/quan0207/miniforge3/envs/deeprestore/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/quan0207/miniforge3/envs/deeprestore/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: /Users/quan0207/miniforge3/envs/deeprestore/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
            "Epoch 1/20 - loss=1.0725 PSNR=29.03 SSIM=0.965\n",
            "Epoch 2/20 - loss=0.8487 PSNR=30.12 SSIM=0.968\n",
            "Epoch 3/20 - loss=0.7988 PSNR=29.73 SSIM=0.968\n",
            "Epoch 4/20 - loss=0.7792 PSNR=30.33 SSIM=0.969\n",
            "Epoch 5/20 - loss=0.7631 PSNR=30.63 SSIM=0.970\n",
            "Epoch 6/20 - loss=0.7384 PSNR=30.54 SSIM=0.970\n",
            "Epoch 7/20 - loss=0.7383 PSNR=30.53 SSIM=0.970\n",
            "Epoch 8/20 - loss=0.7252 PSNR=30.62 SSIM=0.970\n",
            "Epoch 9/20 - loss=0.7201 PSNR=30.75 SSIM=0.970\n",
            "Epoch 10/20 - loss=0.7159 PSNR=30.51 SSIM=0.970\n",
            "Epoch 11/20 - loss=0.7052 PSNR=30.57 SSIM=0.970\n",
            "Epoch 12/20 - loss=0.7037 PSNR=30.95 SSIM=0.971\n",
            "Epoch 13/20 - loss=0.6868 PSNR=30.66 SSIM=0.971\n",
            "Epoch 14/20 - loss=0.6839 PSNR=30.77 SSIM=0.970\n",
            "Epoch 15/20 - loss=0.6757 PSNR=30.73 SSIM=0.971\n",
            "Epoch 16/20 - loss=0.6638 PSNR=31.01 SSIM=0.971\n",
            "Epoch 17/20 - loss=0.6705 PSNR=30.87 SSIM=0.971\n",
            "Epoch 18/20 - loss=0.6617 PSNR=30.72 SSIM=0.971\n",
            "Epoch 19/20 - loss=0.6509 PSNR=31.08 SSIM=0.971\n",
            "Epoch 20/20 - loss=0.6462 PSNR=31.01 SSIM=0.971\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Run training pipeline\n",
        "# -------------------------\n",
        "cfg = TrainConfig()\n",
        "train_dl, val_dl = build_loaders(cfg)\n",
        "\n",
        "model = SimpleUNet(in_ch=4, out_ch=3, base=64).to(device)\n",
        "model.apply(init_weights)\n",
        "D = PatchDiscriminator().to(device)\n",
        "\n",
        "opt_G = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
        "opt_D = torch.optim.AdamW(D.parameters(), lr=cfg.lr * 2, weight_decay=1e-4)\n",
        "\n",
        "lpips_net = init_lpips(device) if cfg.lpips_weight > 0 else None\n",
        "\n",
        "best_ssim = -1.0\n",
        "best_ckpt = 'outputs/inpaint_unet_best.pt'\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    loss = train_one_epoch(model, D, opt_G, opt_D, train_dl, device, lpips_net, cfg.lpips_weight)\n",
        "    psnr, ssim = evaluate_model(model, val_dl, device)\n",
        "    print(f'Epoch {epoch+1}/{cfg.epochs} - loss={loss:.4f} PSNR={psnr:.2f} SSIM={ssim:.3f}')\n",
        "\n",
        "    # lưu best theo SSIM\n",
        "    if ssim > best_ssim:\n",
        "        best_ssim = ssim\n",
        "        torch.save(\n",
        "            {'state_dict': model.state_dict(), 'cfg': asdict(cfg)},\n",
        "            best_ckpt\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4b9505b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sphynx_162.jpg mask counts: {0: 64032, 1: 1504}\n",
            "Bombay_190.jpg mask counts: {0: 63880, 1: 1656}\n",
            "newfoundland_171.jpg mask counts: {0: 64032, 1: 1504}\n",
            "basset_hound_187.jpg mask counts: {0: 60824, 1: 4712}\n",
            "Bengal_117.jpg mask counts: {0: 63802, 1: 1734}\n",
            "newfoundland_2.jpg mask counts: {0: 63096, 1: 2440}\n",
            "boxer_79.jpg mask counts: {0: 63891, 1: 1645}\n",
            "pomeranian_175.jpg mask counts: {0: 61047, 1: 4489}\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /Users/quan0207/miniforge3/envs/deeprestore/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
            "Final metrics → PSNR: 31.01, SSIM: 0.971, LPIPS: 0.037\n",
            "Saved last model to outputs/inpaint_unet_final.pt\n",
            "Best model (by SSIM) saved to outputs/inpaint_unet_best.pt\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Save a few qualitative results\n",
        "# -------------------------\n",
        "saved = 0\n",
        "model.eval()\n",
        "for batch in val_dl:\n",
        "    x = batch['input'].to(device)\n",
        "    y = batch['target'].to(device)\n",
        "    occ_rgb = batch.get('occ_rgb', x[:, :3]).to(device)\n",
        "    names = batch['name']\n",
        "    with torch.no_grad():\n",
        "        y_hat = model(x)\n",
        "        m = batch['mask'].to(device)\n",
        "        final = y_hat * m + x[:, :3] * (1 - m)\n",
        "    B = x.size(0)\n",
        "    for i in range(B):\n",
        "        occ = tensor_to_pil(occ_rgb[i])\n",
        "        pred = tensor_to_pil(final[i])\n",
        "        gt = tensor_to_pil(y[i])\n",
        "\n",
        "        raw_pred = tensor_to_pil(y_hat[i])\n",
        "        mask_img = tensor_to_pil(m[i])\n",
        "        _u, _c = torch.unique(\n",
        "            m[i].round().to(torch.int32), return_counts=True\n",
        "        )\n",
        "        print(names[i], 'mask counts:',\n",
        "              dict(zip(_u.tolist(), [int(x) for x in _c.tolist()])))\n",
        "\n",
        "        occ.save(os.path.join('outputs', f'occ_{names[i]}'))\n",
        "        pred.save(os.path.join('outputs', f'pred_{names[i]}'))\n",
        "        raw_pred.save(os.path.join('outputs', f'debug_raw_{names[i]}'))\n",
        "        mask_img.save(os.path.join('outputs', f'debug_mask_{names[i]}'))\n",
        "        gt.save(os.path.join('outputs', f'gt_{names[i]}'))\n",
        "        saved += 1\n",
        "        if saved >= 8:\n",
        "            break\n",
        "    if saved >= 8:\n",
        "        break\n",
        "\n",
        "# -------------------------\n",
        "# Final metrics (PSNR / SSIM / LPIPS)\n",
        "# -------------------------\n",
        "psnr, ssim = evaluate_model(model, val_dl, device)\n",
        "try:\n",
        "    lp = lpips.LPIPS(net='alex').to(device).eval() if lpips is not None else None\n",
        "    lpips_vals = []\n",
        "    if lp is not None:\n",
        "        for batch in val_dl:\n",
        "            x = batch['input'].to(device)\n",
        "            y = batch['target'].to(device)\n",
        "            with torch.no_grad():\n",
        "                y_hat = model(x)\n",
        "                m = batch['mask'].to(device)\n",
        "                final = y_hat * m + x[:, :3] * (1 - m)\n",
        "            lpips_vals.append(lp(final * 2 - 1, y * 2 - 1).mean().item())\n",
        "    mean_lpips = float(np.mean(lpips_vals)) if lpips_vals else 0.0\n",
        "except Exception:\n",
        "    mean_lpips = 0.0\n",
        "\n",
        "print(f'Final metrics → PSNR: {psnr:.2f}, SSIM: {ssim:.3f}, LPIPS: {mean_lpips:.3f}')\n",
        "\n",
        "ckpt_path = 'outputs/inpaint_unet_final.pt'\n",
        "torch.save(\n",
        "    {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'cfg': asdict(cfg),\n",
        "    },\n",
        "    ckpt_path\n",
        ")\n",
        "print('Saved last model to', ckpt_path)\n",
        "print('Best model (by SSIM) saved to', best_ckpt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d6276a1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mask sum: 2256.0\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# Inference helper\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def infer_one(\n",
        "    model,\n",
        "    occ_img: Image.Image,\n",
        "    occ_mask: Image.Image,\n",
        "    use_segmentation=False,\n",
        "    resize=256,\n",
        "    debug=False\n",
        "):\n",
        "    # segmentation is disabled for stability\n",
        "    inpaint_mask = occ_mask\n",
        "\n",
        "    occ_img_p, mask_p = preprocess_img_and_mask(occ_img, inpaint_mask)\n",
        "\n",
        "    resize_img = T.Resize((resize, resize),\n",
        "                          interpolation=T.InterpolationMode.BICUBIC)\n",
        "    resize_mask = T.Resize((resize, resize),\n",
        "                           interpolation=T.InterpolationMode.NEAREST)\n",
        "\n",
        "    occ_r = resize_img(occ_img_p)\n",
        "    mask_r = resize_mask(mask_p)\n",
        "\n",
        "    occ_t = img_to_tensor(occ_r)\n",
        "    mask_t = (img_to_tensor(mask_r)[:1] > 0.5).float()\n",
        "\n",
        "    # blur inside mask\n",
        "    occ_blur = occ_t.clone()\n",
        "    occ_blur[:, mask_t[0] > 0.5] = 0.0\n",
        "\n",
        "    x = torch.cat([occ_blur, mask_t], dim=0).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    y_hat = model(x)[0]\n",
        "\n",
        "    mask_t = mask_t.to(device)\n",
        "    occ_t = occ_t.to(device)\n",
        "\n",
        "    final = y_hat * mask_t + occ_t * (1 - mask_t)\n",
        "\n",
        "    out = tensor_to_pil(final)\n",
        "\n",
        "    # resize back to original\n",
        "    if out.size != occ_img.size:\n",
        "        out = out.resize(occ_img.size, Image.BICUBIC)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Mask sum:\", mask_t.sum().item())\n",
        "        tensor_to_pil(mask_t).save(\"debug_mask.png\")\n",
        "        tensor_to_pil(y_hat).save(\"debug_raw.png\")\n",
        "        tensor_to_pil(final).save(\"debug_final.png\")\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Load best checkpoint & test\n",
        "# -------------------------\n",
        "ckpt = torch.load('outputs/inpaint_unet_best.pt', map_location=device)\n",
        "cfg_loaded = ckpt[\"cfg\"]\n",
        "\n",
        "best_model = SimpleUNet(in_ch=4, out_ch=3, base=64).to(device)\n",
        "best_model.load_state_dict(ckpt[\"state_dict\"])\n",
        "best_model.eval()\n",
        "\n",
        "# 2) Load input images (example paths; đổi sang path của bạn)\n",
        "occ = Image.open(\"/Users/quan0207/School/Computer vision/DeepRestore/data/occluded/yorkshire_terrier_8.jpg\")\n",
        "mask = Image.open(\"/Users/quan0207/School/Computer vision/DeepRestore/data/occluded/yorkshire_terrier_8_mask.png\")\n",
        "\n",
        "# 3) Inference\n",
        "res = infer_one(best_model, occ, mask, use_segmentation=False, debug=True)\n",
        "res.save(\"final_result.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deeprestore",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
